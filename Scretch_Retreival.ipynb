{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eunji/.pyenv/versions/3.10.9/envs/tableqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from easydict import EasyDict\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "import os\n",
    "from transformers import EarlyStoppingCallback\n",
    "import random\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 7\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"None\", \"Max\", \"Min\", \"Count\", \"Sum\", \"Average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_labels = len(labels)\n",
    "id2label = {id:label for id, label in enumerate(labels)}\n",
    "label2id = {label:id for id, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'None': 0, 'Max': 1, 'Min': 2, 'Count': 3, 'Sum': 4, 'Average': 5}\n",
      "{0: 'None', 1: 'Max', 2: 'Min', 3: 'Count', 4: 'Sum', 5: 'Average'}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"wikisql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(datasets['train'])\n",
    "val = pd.DataFrame(datasets['validation'])\n",
    "test = pd.DataFrame(datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg = train['sql'].apply(lambda x: x['agg']).tolist()\n",
    "test_agg = test['sql'].apply(lambda x: x['agg']).tolist()\n",
    "val_agg = val['sql'].apply(lambda x: x['agg']).tolist()\n",
    "train_query = train['question'].tolist()\n",
    "train_header = train['table'].apply(lambda x: x['header']).tolist()\n",
    "test_query = test['question'].tolist()\n",
    "test_header = test['table'].apply(lambda x: x['header']).tolist()\n",
    "val_query = val['question'].tolist()\n",
    "val_header = val['table'].apply(lambda x: x['header']).tolist()\n",
    "train_qt = [id2label[x] for x in train_agg]\n",
    "val_qt = [id2label[x] for x in val_agg]\n",
    "test_qt = [id2label[x] for x in test_agg]\n",
    "\n",
    "train_data = pd.DataFrame({'query': train_query, 'header': train_header, 'agg': train_agg, 'agg_label': train_qt})\n",
    "test_data = pd.DataFrame({'query': test_query, 'header': test_header, 'agg': test_agg, 'agg_label': test_qt})\n",
    "val_data = pd.DataFrame({'query': val_query, 'header': val_header, 'agg': val_agg, 'agg_label': val_qt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>header</th>\n",
       "      <th>agg</th>\n",
       "      <th>agg_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me what the notes are for South Australia</td>\n",
       "      <td>[State/territory, Text/background colour, Form...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the current series where the new serie...</td>\n",
       "      <td>[State/territory, Text/background colour, Form...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the format for South Australia?</td>\n",
       "      <td>[State/territory, Text/background colour, Form...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Name the background colour for the Australian ...</td>\n",
       "      <td>[State/territory, Text/background colour, Form...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how many times is the fuel propulsion is cng?</td>\n",
       "      <td>[Order Year, Manufacturer, Model, Fleet Series...</td>\n",
       "      <td>3</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0    Tell me what the notes are for South Australia    \n",
       "1  What is the current series where the new serie...   \n",
       "2            What is the format for South Australia?   \n",
       "3  Name the background colour for the Australian ...   \n",
       "4      how many times is the fuel propulsion is cng?   \n",
       "\n",
       "                                              header  agg agg_label  \n",
       "0  [State/territory, Text/background colour, Form...    0      None  \n",
       "1  [State/territory, Text/background colour, Form...    0      None  \n",
       "2  [State/territory, Text/background colour, Form...    0      None  \n",
       "3  [State/territory, Text/background colour, Form...    0      None  \n",
       "4  [Order Year, Manufacturer, Model, Fleet Series...    3     Count  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>agg</th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.090746</td>\n",
       "      <td>0.057333</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>0.036235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.713188</td>\n",
       "      <td>0.091636</td>\n",
       "      <td>0.058761</td>\n",
       "      <td>0.061280</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>0.036969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.714523</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.055575</td>\n",
       "      <td>0.060207</td>\n",
       "      <td>0.039069</td>\n",
       "      <td>0.038119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "agg           0         3         2         1         5         4\n",
       "train  0.720539  0.090746  0.057333  0.056091  0.039056  0.036235\n",
       "test   0.713188  0.091636  0.058761  0.061280  0.038166  0.036969\n",
       "val    0.714523  0.092507  0.055575  0.060207  0.039069  0.038119"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agg_ratio(df):\n",
    "    return df['agg'].value_counts() / df['agg'].value_counts().sum()\n",
    "\n",
    "train_agg_ratio = agg_ratio(train_data)\n",
    "test_agg_ratio = agg_ratio(test_data)\n",
    "val_agg_ratio = agg_ratio(val_data)\n",
    "\n",
    "# 기존 데이터셋\n",
    "pd.DataFrame([train_agg_ratio, test_agg_ratio, val_agg_ratio], index=['train', 'test', 'val']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agg\n",
       "0    40606\n",
       "3     5114\n",
       "2     3231\n",
       "1     3161\n",
       "5     2201\n",
       "4     2042\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['agg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5635.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df, target_col='agg', target_class=0, target_ratio=0.1, random_state=42):\n",
    "    # 클래스별로 분리\n",
    "    majority = df[df[target_col] == target_class]\n",
    "    others = df[df[target_col] != target_class]\n",
    "\n",
    "    # 목표 비율에 맞게 클래스 0에서 일부만 샘플링\n",
    "    target_n = int(len(df) * target_ratio)\n",
    "    sampled_majority = majority.sample(n=target_n, random_state=random_state)\n",
    "\n",
    "    # 합치기\n",
    "    balanced_df = pd.concat([sampled_majority, others], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True) # frac=1 → 전체 행을 다 섞음 (shuffle)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resampled = undersample(train_data) \n",
    "test_resampled = undersample(test_data)\n",
    "val_resampled = undersample(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[agg\n",
       " 0    5635\n",
       " 3    5114\n",
       " 2    3231\n",
       " 1    3161\n",
       " 5    2201\n",
       " 4    2042\n",
       " Name: count, dtype: int64,\n",
       " agg\n",
       " 0    1587\n",
       " 3    1455\n",
       " 1     973\n",
       " 2     933\n",
       " 5     606\n",
       " 4     587\n",
       " Name: count, dtype: int64,\n",
       " agg\n",
       " 0    842\n",
       " 3    779\n",
       " 1    507\n",
       " 2    468\n",
       " 5    329\n",
       " 4    321\n",
       " Name: count, dtype: int64]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df['agg'].value_counts() for df in [train_resampled, test_resampled, val_resampled]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 쉽게 처리하기 위해 json 파일로 저장\n",
    "def convert_to_jsonl(df, out_path):\n",
    "    with gzip.open(out_path, 'wt', encoding='utf-8') as f:\n",
    "        for i in range(len(df)):\n",
    "\n",
    "            # header안에 리스트인 경우 문자로 변환해서 *로 합쳐줘야함 \n",
    "            header = df['header'][i]\n",
    "            if isinstance(header, list):\n",
    "                header = [str(h) for h in header]\n",
    "            else:\n",
    "                header = str(header)\n",
    "\n",
    "            # 라벨값이 numpy이면 json.dumps가 처리하지 못함\n",
    "            label = df['agg'][i]\n",
    "            if isinstance(label, (np.integer, np.int64, np.int32)):\n",
    "                label = int(label)\n",
    "\n",
    "            item = {\n",
    "                'id': i,\n",
    "                'query': df['query'][i],\n",
    "                'header': ' * '.join(header),\n",
    "                'label': label,\n",
    "                'category' : df['agg_label'][i]\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_sep_token(df):\n",
    "    # 각 행의 header가 리스트면 문자열로 변환하고 *로 연결\n",
    "    new_headers = []\n",
    "    for header in df['header']:\n",
    "        if isinstance(header, list):\n",
    "            header = [str(h) for h in header]\n",
    "            header = ' * '.join(header)\n",
    "        else:\n",
    "            header = str(header)\n",
    "        new_headers.append(header)\n",
    "    df['header'] = new_headers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/eunji/workspace/kim-internship/Eunji/wikisql_jsonl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_jsonl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# os.path.join(디렉토리, 파일명)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mconvert_to_jsonl\u001b[49m(train_resampled, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.jsonl.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m convert_to_jsonl(test_resampled, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.jsonl.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m convert_to_jsonl(val_resampled, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval.jsonl.gz\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_to_jsonl' is not defined"
     ]
    }
   ],
   "source": [
    "# os.path.join(디렉토리, 파일명)\n",
    "convert_to_jsonl(train_resampled, os.path.join(file_path, 'train.jsonl.gz'))\n",
    "convert_to_jsonl(test_resampled, os.path.join(file_path, 'test.jsonl.gz'))\n",
    "convert_to_jsonl(val_resampled, os.path.join(file_path, 'val.jsonl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 0, \"query\": \"Name the most attendance for giants points of 10\", \"header\": \"Game * Date * Opponent * Result * Giants points * Opponents * Record * Attendance\", \"label\": 1, \"category\": \"Max\"}\n",
      "\n",
      "{\"id\": 1, \"query\": \"What is the average (Jericho) with a Population (Total) less than 4,059 in 1986 and a (Barcaldine) higher than 1,779?\", \"header\": \"Year * Population (Total) * (Barcaldine) * (Aramac) * (Jericho)\", \"label\": 5, \"category\": \"Average\"}\n",
      "\n",
      "{\"id\": 2, \"query\": \"Which studio grossed $83,531,958 and ranked lower than 13?\", \"header\": \"Rank * Title * Studio * Director(s) * Gross\", \"label\": 0, \"category\": \"None\"}\n",
      "\n",
      "{\"id\": 3, \"query\": \"What are the fewest number of podiums associated with a Series of formula renault 2000 brazil, and under 1 pole?\", \"header\": \"Season * Series * Races * Poles * Wins * Podiums * Points * Position\", \"label\": 2, \"category\": \"Min\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# jsonl.gz 파일은 압축되어 있음\n",
    "# 압축을 풀어서 확인\n",
    "\n",
    "file_check = os.path.join(file_path + 'test.jsonl.gz')\n",
    "with gzip.open(file_check, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line)\n",
    "        if i > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files={\n",
    "    'train': os.path.join(file_path, 'train.jsonl.gz'),\n",
    "    'validation': os.path.join(file_path,'val.jsonl.gz'),\n",
    "    'test': os.path.join(file_path ,'test.jsonl.gz')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre_encoding = header_sep_token(train_resampled)\n",
    "test_pre_encoding = header_sep_token(test_resampled)\n",
    "val_pre_encoding = header_sep_token(val_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Team * Points * Played * Drawn * Lost * Agains...\n",
       "1           Rank * Player * Country * Earnings( $ ) * Wins\n",
       "2        Week * Date * Kickoff * Opponent * Final score...\n",
       "3                    Rank * Gold * Silver * Bronze * Total\n",
       "4        Lot No. * Diagram * Built * Builder * Fleet nu...\n",
       "                               ...                        \n",
       "21379    Driver * Constructor * Laps * Time/Retired * Grid\n",
       "21380    Matches * Innings * Not out * Runs * High Scor...\n",
       "21381          Team * City * State * Home venue * Capacity\n",
       "21382    Game * Date * Team * Score * High points * Hig...\n",
       "21383    Season * Series * Races * Wins * Poles * F/Lap...\n",
       "Name: header, Length: 21384, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pre_encoding['header']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eunji/.pyenv/versions/3.10.9/envs/tableqa/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(example['query'], example['header'], \n",
    "                     #return_tensors='pt', \n",
    "                     truncation=True, \n",
    "                     padding='max_length', # 최대길이가 안되면 나머지 0으로 채움\n",
    "                     max_length=128) # 문장 최대 길이\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eunji/.pyenv/versions/3.10.9/envs/tableqa/lib/python3.10/site-packages/transformers/modeling_utils.py:371: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=6, id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# agg_index는 0~5\n",
    "# 분류 문제 → CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_resampled['agg']\n",
    "val_labels = val_resampled['agg']\n",
    "test_labels = test_resampled['agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
    "            item[key] = torch.tensor(self.encodings[key][idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader  = DataLoader(tokenized_dataset['train'], list(train_labels))\n",
    "val_dataloader  = DataLoader(tokenized_dataset['validation'], list(val_labels))\n",
    "test_dataset  = DataLoader(tokenized_dataset['test'], list(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 2054, 2003, 1996, 3284, 4487, 4246, 2007, 4567, 1997, 1020, 1998,\n",
       "         2377, 3469, 2084, 2324, 1029,  102, 2136, 1008, 2685, 1008, 2209, 1008,\n",
       "         4567, 1008, 2439, 1008, 2114, 1008, 4487, 4246,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred = (predictions, labels)\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 다중분류\n",
    "    # 특정 i라벨의 확률 = 특정 i 라벨의 승산/모든 라벨의 승산 \n",
    "    # predictions = [batch_size, num_labels]\n",
    "    # probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "\n",
    "    # 가장 로짓이 큰 라벨 추출\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # compute accuracy와 f1-score\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, \n",
    "                                     references=labels)['accuracy'],3)\n",
    "    \n",
    "    f1 = np.round(f1_score.compute(predictions=predicted_classes, references=labels, average='macro')['f1'], 3) #  라벨별 f1-score를 산술평균한 것 : 현재 라벨의 갯수가 같아서 이렇게 써도 된다고 판단\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters\n",
    "lr = 4e-5\n",
    "num_epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(file_path,'bert-agg'),\n",
    "    logging_dir=os.path.join(file_path,'logs'),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=num_epochs,\n",
    "    #learning_rate=lr,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    #logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #eval_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    fp16=True,\n",
    "    #metric_for_best_model=\"f1\",\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader,\n",
    "    eval_dataset=val_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_dataset['test'])\n",
    "pred_labels = preds.predictions.argmax(-1)\n",
    "\n",
    "# 실제값 (리스트 → 배열로 변환)\n",
    "true_label = np.array(tokenized_dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "accuracy = accuracy_score(true_label, pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_label, pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tableqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
