{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)  # 최대 1000개 행 출력 허용\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_words(df, dataname, min_count=10, language='english'):\n",
    "    if dataname == 'sql':\n",
    "        text_col='query'\n",
    "    else: \n",
    "        text_col='question'\n",
    "    stop_words = set(stopwords.words(language))\n",
    "\n",
    "    # 모든 문장을 하나로 합친 뒤 소문자 변환 + 특수문자 제거\n",
    "    text = ' '.join(df[text_col]).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 특수문자 제거\n",
    "\n",
    "    # 토큰화 후 불용어 제거\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # 단어 개수 세기\n",
    "    word_counts = pd.Series(words).value_counts()\n",
    "    return word_counts[word_counts >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def annotate_question(question):\n",
    "    q_lower = question.lower()\n",
    "\n",
    "    # 일반적인 'lookup' 조건 \n",
    "    if re.search(r'\\b(which|what|who|name)\\b', q_lower):\n",
    "        return 'Lookup'\n",
    "    \n",
    "    # WikiTQ 데이터셋에서 추가된\n",
    "    # 'where'과 'first'가 동시에 있을 때\n",
    "    if 'when' in q_lower and 'first' in q_lower:\n",
    "        return 'Lookup'\n",
    "    \n",
    "    if re.search(r'\\b(where)\\b', q_lower):\n",
    "        return 'Lookup'\n",
    "\n",
    "    # 일반적인 'Aggregation' 조건 \n",
    "    if re.search(r'\\bhow many\\b', q_lower) or \\\n",
    "       re.search(r'\\b(total|sum|count|average)\\b', q_lower):\n",
    "        return 'Aggregation'\n",
    "    \n",
    "    # WikiTQ 데이터셋에서 추가된 Aggregation 조건 \n",
    "    if re.search(r'\\b(highest|lowest|most|least|fastest|slowest|largest|fewest)\\b', q_lower):\n",
    "        return 'Aggregation'\n",
    "    \n",
    "    if re.search(r'\\bnumber of\\b', q_lower):\n",
    "        return 'Aggregation'\n",
    "    \n",
    "    if re.search(r'\\b(is|were|was|does|did)\\b', q_lower):\n",
    "        return 'Aggregation'\n",
    "    \n",
    "    if re.search(r'\\bmore or less\\b', q_lower):\n",
    "        return 'Aggregation'\n",
    "    \n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "datasets = load_dataset(\"wikitablequestions\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(datasets['train'])\n",
    "val = pd.DataFrame(datasets['validation'])\n",
    "test = pd.DataFrame(datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11321\n",
      "4344\n",
      "2831\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"neulab/omnitab-large\")\n",
    "\n",
    "# 결과 저장용 딕셔너리\n",
    "results = {}\n",
    "over_1024 = {}   \n",
    "under_1024 = {}\n",
    "\n",
    "# 각 데이터셋(train, test, validation)에 대해 처리\n",
    "for split in [\"train\", \"test\", \"validation\"]:\n",
    "    data = datasets[split]\n",
    "\n",
    "    # 각 테이블의 토큰 개수 계산\n",
    "    token_counts = []\n",
    "    under_1024[split] = []\n",
    "    over_1024[split] = []  # split별 리스트 초기화\n",
    "\n",
    "    for sample in data:\n",
    "        table = sample[\"table\"]  # 테이블 데이터 가져오기\n",
    "\n",
    "        # Pandas DataFrame 변환\n",
    "        df_table = pd.DataFrame(table[\"rows\"], columns=table[\"header\"])\n",
    "\n",
    "        # TAPEX(OmniTab)는 DataFrame과 질문을 함께 입력받아야 함\n",
    "        tokenized = tokenizer(table=df_table, query=sample[\"question\"], truncation=False)\n",
    "\n",
    "        # input_ids가 리스트인지 확인 후 길이 측정\n",
    "        token_count = len(tokenized[\"input_ids\"]) if isinstance(tokenized[\"input_ids\"], list) else tokenized[\"input_ids\"]\n",
    "        token_counts.append(token_count)\n",
    "\n",
    "        # 1024 초과인 경우 저장\n",
    "        if token_count > 1024:\n",
    "            over_1024[split].append(sample)\n",
    "        else: \n",
    "            under_1024[split].append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1987, 'test': 607, 'validation': 496}\n",
      "{'train': 9334, 'test': 3737, 'validation': 2335}\n"
     ]
    }
   ],
   "source": [
    "print({k: len(v) for k, v in over_1024.items()})\n",
    "print({k: len(v) for k, v in under_1024.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "over_train = random.sample(over_1024['train'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'C:/Users/B6313/tableqa_wiki/wikiTQ_json'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    # over_1024 저장\n",
    "    over_path = os.path.join(output_dir, f'over_{split}.json')\n",
    "    with open(over_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(over_1024[split], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # under_1024 저장\n",
    "    under_path = os.path.join(output_dir, f'under_{split}.json')\n",
    "    with open(under_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(under_1024[split], f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_1024 ={}\n",
    "under_1024 = {}\n",
    "\n",
    "over_1024_df = {}\n",
    "under_1024_df = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    # over_1024 불러오기\n",
    "    over_path = os.path.join(output_dir, f'over_{split}.json')\n",
    "    with open(over_path, 'r', encoding='utf-8') as f:\n",
    "        over_1024[split] = json.load(f)\n",
    "        over_1024_df[split] = pd.DataFrame(over_1024[split])\n",
    "    \n",
    "    # under_1024 불러오기\n",
    "    under_path = os.path.join(output_dir, f'under_{split}.json')\n",
    "    with open(under_path, 'r', encoding='utf-8') as f:\n",
    "        under_1024[split] = json.load(f)\n",
    "        under_1024_df[split] = pd.DataFrame(under_1024[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[under_1024] train: \n",
      "annotation\n",
      "Lookup         5917\n",
      "Aggregation    3235\n",
      "Other           182\n",
      "Name: count, dtype: int64\n",
      "[under_1024] validation: \n",
      "annotation\n",
      "Lookup         1454\n",
      "Aggregation     813\n",
      "Other            68\n",
      "Name: count, dtype: int64\n",
      "[under_1024] test: \n",
      "annotation\n",
      "Lookup         2328\n",
      "Aggregation    1319\n",
      "Other            90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    under_1024_df[split]['annotation'] = under_1024_df[split]['question'].apply(annotate_question)\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"[under_1024] {split}: \\n{under_1024_df[split]['annotation'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>question</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'header': ['Rank', 'Cyclist', 'Team', 'Time',...</td>\n",
       "      <td>which country had the most cyclists finish wit...</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'header': ['Description Losses', '1939/40', '...</td>\n",
       "      <td>how many people were murdered in 1940/41?</td>\n",
       "      <td>Aggregation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'header': ['Year', 'Division', 'League', 'Reg...</td>\n",
       "      <td>how long did it take for the new york american...</td>\n",
       "      <td>Aggregation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'header': ['Series #', 'Season #', 'Title', '...</td>\n",
       "      <td>alfie's birthday party aired on january 19. wh...</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'header': ['Date', 'Competition', 'Location',...</td>\n",
       "      <td>what is the number of 1st place finishes acros...</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'header': ['Year', 'Competition', 'Venue', 'P...</td>\n",
       "      <td>in which competition did hopley finish fist?</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'header': ['Year', 'Film', 'Role', 'Language'...</td>\n",
       "      <td>what is the total number of films with the lan...</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'header': ['Game', 'Day', 'Date', 'Kickoff', ...</td>\n",
       "      <td>what was the number of people attending the to...</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'header': ['Year', 'Kit Manufacturer', 'Shirt...</td>\n",
       "      <td>what time period had no shirt sponsor?</td>\n",
       "      <td>Lookup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'header': ['Year', 'Competition', 'Venue', 'P...</td>\n",
       "      <td>when was his first 1st place record?</td>\n",
       "      <td>Aggregation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               table  \\\n",
       "0  {'header': ['Rank', 'Cyclist', 'Team', 'Time',...   \n",
       "1  {'header': ['Description Losses', '1939/40', '...   \n",
       "2  {'header': ['Year', 'Division', 'League', 'Reg...   \n",
       "3  {'header': ['Series #', 'Season #', 'Title', '...   \n",
       "4  {'header': ['Date', 'Competition', 'Location',...   \n",
       "5  {'header': ['Year', 'Competition', 'Venue', 'P...   \n",
       "6  {'header': ['Year', 'Film', 'Role', 'Language'...   \n",
       "7  {'header': ['Game', 'Day', 'Date', 'Kickoff', ...   \n",
       "8  {'header': ['Year', 'Kit Manufacturer', 'Shirt...   \n",
       "9  {'header': ['Year', 'Competition', 'Venue', 'P...   \n",
       "\n",
       "                                            question   annotation  \n",
       "0  which country had the most cyclists finish wit...       Lookup  \n",
       "1          how many people were murdered in 1940/41?  Aggregation  \n",
       "2  how long did it take for the new york american...  Aggregation  \n",
       "3  alfie's birthday party aired on january 19. wh...       Lookup  \n",
       "4  what is the number of 1st place finishes acros...       Lookup  \n",
       "5       in which competition did hopley finish fist?       Lookup  \n",
       "6  what is the total number of films with the lan...       Lookup  \n",
       "7  what was the number of people attending the to...       Lookup  \n",
       "8             what time period had no shirt sponsor?       Lookup  \n",
       "9               when was his first 1st place record?  Aggregation  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_1024_df['test'][['table', 'question', 'annotation']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under_1024_df['train'][under_1024_df['train']['annotation'].str.contains('Other', case=False, na=False)][10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filtered_under_1024_df] train: \n",
      "annotation\n",
      "Lookup         5917\n",
      "Aggregation    3235\n",
      "Name: count, dtype: int64\n",
      "[filtered_under_1024_df] validation: \n",
      "annotation\n",
      "Lookup         1454\n",
      "Aggregation     813\n",
      "Name: count, dtype: int64\n",
      "[filtered_under_1024_df] test: \n",
      "annotation\n",
      "Lookup         2328\n",
      "Aggregation    1319\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filtered_under_1024_df = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    df = under_1024_df[split]\n",
    "    filtered_under_1024_df[split] = df[df['annotation'].isin(['Lookup', 'Aggregation'])]\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"[filtered_under_1024_df] {split}: \\n{filtered_under_1024_df[split]['annotation'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 라벨 None을 'annotation'열에서 Lookup으로 바꾸기\n",
    "2. Lookup을 제외한 나머지 라벨은 'annotation'열에서 Aggregation으로 바꾸기\n",
    "3. 개수 차이나는지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql = load_dataset(\"wikisql\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 15878, 'validation': 8421, 'train': 56355}\n"
     ]
    }
   ],
   "source": [
    "print({k: len(v) for k, v in wikisql.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql_df = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    wikisql_df[split] = pd.DataFrame(wikisql[split])\n",
    "    wikisql_df[split]['annotation'] = wikisql_df[split]['sql'].apply(\n",
    "        lambda x: 'Lookup' if x['agg'] == 0 else 'Aggregation'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wikisql_df] train: \n",
      "annotation\n",
      "Lookup         40606\n",
      "Aggregation    15749\n",
      "Name: count, dtype: int64\n",
      "[wikisql_df] validation: \n",
      "annotation\n",
      "Lookup         6017\n",
      "Aggregation    2404\n",
      "Name: count, dtype: int64\n",
      "[wikisql_df] test: \n",
      "annotation\n",
      "Lookup         11324\n",
      "Aggregation     4554\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"[wikisql_df] {split}: \\n{wikisql_df[split]['annotation'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 개수 차이가 나므로 비율 정해서 언더샘플링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]\n",
      "annotation\n",
      "Lookup         0.720539\n",
      "Aggregation    0.279461\n",
      "Name: count, dtype: float64\n",
      "\n",
      "[validation]\n",
      "annotation\n",
      "Lookup         0.714523\n",
      "Aggregation    0.285477\n",
      "Name: count, dtype: float64\n",
      "\n",
      "[test]\n",
      "annotation\n",
      "Lookup         0.713188\n",
      "Aggregation    0.286812\n",
      "Name: count, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def agg_ratio(df):\n",
    "    return df['annotation'].value_counts() / df['annotation'].value_counts().sum()\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"[{split}]\")\n",
    "    print(agg_ratio(wikisql_df[split]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df, target_col='annotation', target_class='Lookup', target_ratio=0.35, random_state=7):\n",
    "    # 클래스별로 분리\n",
    "    majority = df[df[target_col] == target_class]\n",
    "    others = df[df[target_col] != target_class]\n",
    "\n",
    "    # 목표 비율에 맞게 클래스 'Lookup'에서 일부만 샘플링\n",
    "    target_n = int(len(df) * target_ratio)\n",
    "    #print(f'traget_n :{target_n}')\n",
    "    sampled_majority = majority.sample(n=target_n, random_state=random_state)\n",
    "\n",
    "    # 합치기\n",
    "    balanced_df = pd.concat([sampled_majority, others], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True) # frac=1 → 전체 행을 다 섞음 (shuffle)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql_balanced_df = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    wikisql_balanced_df[split] = undersample(wikisql_df[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]\n",
      "annotation\n",
      "Lookup         19724\n",
      "Aggregation    15749\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "[validation]\n",
      "annotation\n",
      "Lookup         2947\n",
      "Aggregation    2404\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "[test]\n",
      "annotation\n",
      "Lookup         5557\n",
      "Aggregation    4554\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"[{split}]\")\n",
    "    print(wikisql_balanced_df[split]['annotation'].value_counts().round(2), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. wikisql 데이터와 wikiTableQuestion 데이터 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'question', 'answers', 'table', 'annotation'], dtype='object')\n",
      "Index(['phase', 'question', 'table', 'sql', 'annotation'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(filtered_under_1024_df['train'].columns)\n",
    "print(wikisql_balanced_df['train'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    df1 = filtered_under_1024_df[split][['question', 'table', 'annotation']].copy()\n",
    "    df2 = wikisql_balanced_df[split][['question', 'table', 'annotation']].copy()\n",
    "\n",
    "    # 필요하다면 source 구분\n",
    "    df1['source'] = 'wikitq'\n",
    "    df2['source'] = 'wikisql'\n",
    "\n",
    "    for df in [df1, df2]:\n",
    "        df['annotation_num'] = df['annotation'].map({'Lookup':0, 'Aggregation':1})\n",
    "\n",
    "    combined[split] = pd.concat([df1, df2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    combined[split]['header'] = combined[split]['table'].apply(lambda x: x['header'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'validation', 'test']:\n",
    "    file_path = os.path.join(output_dir, f'combined_{split}.json')\n",
    "    combined[split].to_json(file_path, force_ascii=False, orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    file_path = os.path.join(output_dir, f'combined_{split}.json')\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        combined[split] = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table</th>\n",
       "      <th>annotation</th>\n",
       "      <th>source</th>\n",
       "      <th>annotation_num</th>\n",
       "      <th>header</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which team won previous to crettyard?</td>\n",
       "      <td>{'header': ['Team', 'County', 'Wins', 'Years w...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikitq</td>\n",
       "      <td>0</td>\n",
       "      <td>[Team, County, Wins, Years won]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how many more passengers flew to los angeles t...</td>\n",
       "      <td>{'header': ['Rank', 'City', 'Passengers', 'Ran...</td>\n",
       "      <td>Aggregation</td>\n",
       "      <td>wikitq</td>\n",
       "      <td>1</td>\n",
       "      <td>[Rank, City, Passengers, Ranking, Airline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>after winning on four credits with a full hous...</td>\n",
       "      <td>{'header': ['Hand', '1 credit', '2 credits', '...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikitq</td>\n",
       "      <td>0</td>\n",
       "      <td>[Hand, 1 credit, 2 credits, 3 credits, 4 credi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which players played the same position as ardo...</td>\n",
       "      <td>{'header': ['No.', 'Player', 'Birth Date', 'We...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikitq</td>\n",
       "      <td>0</td>\n",
       "      <td>[No., Player, Birth Date, Weight, Height, Posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what was the venue when he placed first?</td>\n",
       "      <td>{'header': ['Year', 'Competition', 'Venue', 'P...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikitq</td>\n",
       "      <td>0</td>\n",
       "      <td>[Year, Competition, Venue, Position, Notes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>How much Gain has a Long of 29, and an Avg/G s...</td>\n",
       "      <td>{'header': ['Name', 'Gain', 'Loss', 'Long', 'A...</td>\n",
       "      <td>Aggregation</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>1</td>\n",
       "      <td>[Name, Gain, Loss, Long, Avg/G]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>What is the chapter for Illinois Wesleyan?</td>\n",
       "      <td>{'header': ['Chapter', 'Installation Date', 'I...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>0</td>\n",
       "      <td>[Chapter, Installation Date, Institution, Loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>What is the score when the tie is 9?</td>\n",
       "      <td>{'header': ['Tie no', 'Home team', 'Score', 'A...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>0</td>\n",
       "      <td>[Tie no, Home team, Score, Away team, Date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7616</th>\n",
       "      <td>Name the D 47 when it has a D 45 of d 32</td>\n",
       "      <td>{'header': ['D 48', 'D 47', 'D 46', 'D 45', 'D...</td>\n",
       "      <td>Lookup</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>0</td>\n",
       "      <td>[D 48, D 47, D 46, D 45, D 44, D 43, D 42, D 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7617</th>\n",
       "      <td>What is the highest Loss, when Long is greater...</td>\n",
       "      <td>{'header': ['Name', 'Gain', 'Loss', 'Long', 'A...</td>\n",
       "      <td>Aggregation</td>\n",
       "      <td>wikisql</td>\n",
       "      <td>1</td>\n",
       "      <td>[Name, Gain, Loss, Long, Avg/g]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7618 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                 which team won previous to crettyard?   \n",
       "1     how many more passengers flew to los angeles t...   \n",
       "2     after winning on four credits with a full hous...   \n",
       "3     which players played the same position as ardo...   \n",
       "4              what was the venue when he placed first?   \n",
       "...                                                 ...   \n",
       "7613  How much Gain has a Long of 29, and an Avg/G s...   \n",
       "7614        What is the chapter for Illinois Wesleyan?    \n",
       "7615               What is the score when the tie is 9?   \n",
       "7616           Name the D 47 when it has a D 45 of d 32   \n",
       "7617  What is the highest Loss, when Long is greater...   \n",
       "\n",
       "                                                  table   annotation   source  \\\n",
       "0     {'header': ['Team', 'County', 'Wins', 'Years w...       Lookup   wikitq   \n",
       "1     {'header': ['Rank', 'City', 'Passengers', 'Ran...  Aggregation   wikitq   \n",
       "2     {'header': ['Hand', '1 credit', '2 credits', '...       Lookup   wikitq   \n",
       "3     {'header': ['No.', 'Player', 'Birth Date', 'We...       Lookup   wikitq   \n",
       "4     {'header': ['Year', 'Competition', 'Venue', 'P...       Lookup   wikitq   \n",
       "...                                                 ...          ...      ...   \n",
       "7613  {'header': ['Name', 'Gain', 'Loss', 'Long', 'A...  Aggregation  wikisql   \n",
       "7614  {'header': ['Chapter', 'Installation Date', 'I...       Lookup  wikisql   \n",
       "7615  {'header': ['Tie no', 'Home team', 'Score', 'A...       Lookup  wikisql   \n",
       "7616  {'header': ['D 48', 'D 47', 'D 46', 'D 45', 'D...       Lookup  wikisql   \n",
       "7617  {'header': ['Name', 'Gain', 'Loss', 'Long', 'A...  Aggregation  wikisql   \n",
       "\n",
       "      annotation_num                                             header  \n",
       "0                  0                    [Team, County, Wins, Years won]  \n",
       "1                  1         [Rank, City, Passengers, Ranking, Airline]  \n",
       "2                  0  [Hand, 1 credit, 2 credits, 3 credits, 4 credi...  \n",
       "3                  0  [No., Player, Birth Date, Weight, Height, Posi...  \n",
       "4                  0        [Year, Competition, Venue, Position, Notes]  \n",
       "...              ...                                                ...  \n",
       "7613               1                    [Name, Gain, Loss, Long, Avg/G]  \n",
       "7614               0  [Chapter, Installation Date, Institution, Loca...  \n",
       "7615               0        [Tie no, Home team, Score, Away team, Date]  \n",
       "7616               0   [D 48, D 47, D 46, D 45, D 44, D 43, D 42, D 41]  \n",
       "7617               1                    [Name, Gain, Loss, Long, Avg/g]  \n",
       "\n",
       "[7618 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikiTQ, sql 데이터 섞기\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    combined[split] = combined[split].sample(frac=1, random_state=7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 분류기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from easydict import EasyDict\n",
    "import gzip\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "import os\n",
    "from transformers import EarlyStoppingCallback\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 쉽게 처리하기 위해 json 파일로 저장\n",
    "def convert_to_jsonl(df, out_path):\n",
    "    with gzip.open(out_path, 'wt', encoding='utf-8') as f:\n",
    "        for i in range(len(df)):\n",
    "\n",
    "            # header안에 리스트인 경우 문자로 변환해서 *로 합쳐줘야함 \n",
    "            header = df['header'][i]\n",
    "            if isinstance(header, list):\n",
    "                header = [str(h) for h in header]\n",
    "            else:\n",
    "                header = str(header)\n",
    "\n",
    "            # 라벨값이 numpy이면 json.dumps가 처리하지 못함\n",
    "            label = df['annotation_num'][i]\n",
    "            if isinstance(label, (np.integer, np.int64, np.int32)):\n",
    "                label = int(label)\n",
    "\n",
    "            item = {\n",
    "                'id': i,\n",
    "                'query': df['question'][i],\n",
    "                'header': ' * '.join(header),\n",
    "                'label': label,\n",
    "                'category' : df['annotation'][i]\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(example):\n",
    "#     return tokenizer(example['query'], example['header'], \n",
    "#                      #return_tensors='pt', \n",
    "#                      truncation=True, \n",
    "#                      padding='max_length', # 최대길이가 안되면 나머지 0으로 채움\n",
    "#                      max_length=128) # 문장 최대 길이\n",
    "\n",
    "def preprocess(example):\n",
    "    encoding = tokenizer(example['query'], example['header'], truncation=True)\n",
    "    encoding['labels'] = example['label']\n",
    "    return  encoding\n",
    "\n",
    "# def preprocess(example):\n",
    "#     encoding = tokenizer(example['query'], truncation=True)\n",
    "#     encoding['labels'] = example['label']\n",
    "#     return  encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"C:/Users/B6313/tableqa_wiki/wiki_cls_dataset/\"\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    convert_to_jsonl(combined[split], os.path.join(output_path, f'{split}.jsonl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 0, \"query\": \"What was the air date of part 2 of the episode whose part 1 was aired on January 31, 2008?\", \"header\": \"Episode # * Title * Part 1 * Part 2 * Part 3 * Part 4 * Part 5 * Part 6\", \"label\": 0, \"category\": \"Lookup\"}\n",
      "\n",
      "{\"id\": 1, \"query\": \"The candidate who received 133 votes in the Bronx won what percentage overall?\", \"header\": \"1921 * party * Manhattan * The Bronx * Brooklyn * Queens * Richmond [Staten Is.] * Total * %\", \"label\": 0, \"category\": \"Lookup\"}\n",
      "\n",
      "{\"id\": 2, \"query\": \"What is the lowest 2004 population when there were 5158 households?\", \"header\": \"Name * Geographic code * Type * Households * Population (2004) * Foreign population * Moroccan population\", \"label\": 1, \"category\": \"Aggregation\"}\n",
      "\n",
      "{\"id\": 3, \"query\": \"What is the average number of innings with more than 3 in the 100s category?\", \"header\": \"Player * Team * Matches * Innings * Runs * Average * Highest Score * 100s\", \"label\": 1, \"category\": \"Aggregation\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = \"C:/Users/B6313/tableqa_wiki/wiki_cls_dataset/\"\n",
    "file_check = os.path.join(output_path + 'test.jsonl.gz')\n",
    "with gzip.open(file_check, 'rt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line)\n",
    "        if i > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 44625 examples [00:00, 885776.69 examples/s]\n",
      "Generating validation split: 7618 examples [00:00, 441810.92 examples/s]\n",
      "Generating test split: 13758 examples [00:00, 959227.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files={\n",
    "    'train': os.path.join(output_path, 'train.jsonl.gz'),\n",
    "    'validation': os.path.join(output_path,'validation.jsonl.gz'),\n",
    "    'test': os.path.join(output_path ,'test.jsonl.gz')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789.0625"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((44625/8) * 10) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'query': 'name someone else from scotland inducted before alan brazil.',\n",
       " 'header': 'Season * Level * Name * Position * Nationality * International\\\\ncaps',\n",
       " 'label': 0,\n",
       " 'category': 'Lookup'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 44625/44625 [00:01<00:00, 39864.62 examples/s]\n",
      "Map: 100%|██████████| 7618/7618 [00:00<00:00, 36033.18 examples/s]\n",
      "Map: 100%|██████████| 13758/13758 [00:00<00:00, 16387.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = 'google-bert/bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'query': ['which country had the most cyclists finish within the top 10?', 'how many people were murdered in 1940/41?', 'how long did it take for the new york americans to win the national cup after 1936?', \"alfie's birthday party aired on january 19. what was the airdate of the next episode?\", 'what is the number of 1st place finishes across all events?', 'in which competition did hopley finish fist?', 'what is the total number of films with the language of kannada listed?', 'what was the number of people attending the toros mexico vs. monterrey flash game?', 'what time period had no shirt sponsor?', 'when was his first 1st place record?', 'does pat or john have the highest total?', 'what is the combined score of year end rankings before 2009?', 'how many more ships were wrecked in lake huron than in erie?', 'what was the total number of points scored by the tide in the last 3 games combined.', 'who came immediately after sebastian porto in the race?', \"what's the total number of festivals that occurred in october?\", 'what is the total number of skoda cars sold in the year 2005?', 'what was the number of times won on grass?', 'who won the most gold medals?', 'total wins by belgian riders'], 'header': ['Rank * Cyclist * Team * Time * UCI ProTour\\\\nPoints', 'Description Losses * 1939/40 * 1940/41 * 1941/42 * 1942/43 * 1943/44 * 1944/45 * Total', 'Year * Division * League * Reg. Season * Playoffs * National Cup', 'Series # * Season # * Title * Notes * Original air date', 'Date * Competition * Location * Country * Event * Placing * Rider * Nationality', 'Year * Competition * Venue * Position * Event * Notes', 'Year * Film * Role * Language * Notes', 'Game * Day * Date * Kickoff * Opponent * Results\\\\nScore * Results\\\\nRecord * Location * Attendance', 'Year * Kit Manufacturer * Shirt Sponsor * Back of Shirt Sponsor * Short Sponsor', 'Year * Competition * Venue * Position * Event * Notes', 'Name * League * FA Cup * League Cup * JP Trophy * Total', 'Tournament * 2004 * 2005 * 2006 * 2007 * 2008 * 2009 * 2010 * 2011 * 2012 * 2013 * 2014 * W–L', 'Ship * Type of Vessel * Lake * Location * Lives lost', 'Date * Opponent# * Rank# * Site * TV * Result * Attendance', 'Pos * Rider * Manufacturer * Time/Retired * Points', 'Date * Festival * Location * Awards * Link', 'Model * 1991 * 1995 * 1996 * 1997 * 1998 * 1999 * 2000 * 2001 * 2002 * 2003 * 2004 * 2005 * 2006 * 2007 * 2008 * 2009 * 2010 * 2011 * 2012 * 2013', 'Outcome * No. * Date * Championship * Surface * Opponent in the final * Score in the final', 'Rank * Nation * Gold * Silver * Bronze * Total', 'Place * Rider * Country * Team * Points * Wins'], 'label': [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1], 'category': ['Lookup', 'Aggregation', 'Aggregation', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Aggregation', 'Lookup', 'Aggregation', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Lookup', 'Aggregation'], 'input_ids': [[101, 2029, 2406, 2018, 1996, 2087, 21912, 3926, 2306, 1996, 2327, 2184, 1029, 102], [101, 2129, 2116, 2111, 2020, 7129, 1999, 3878, 1013, 4601, 1029, 102], [101, 2129, 2146, 2106, 2009, 2202, 2005, 1996, 2047, 2259, 4841, 2000, 2663, 1996, 2120, 2452, 2044, 4266, 1029, 102], [101, 24493, 2666, 1005, 1055, 5798, 2283, 4836, 2006, 2254, 2539, 1012, 2054, 2001, 1996, 2250, 13701, 1997, 1996, 2279, 2792, 1029, 102], [101, 2054, 2003, 1996, 2193, 1997, 3083, 2173, 12321, 2408, 2035, 2824, 1029, 102], [101, 1999, 2029, 2971, 2106, 6154, 3051, 3926, 7345, 1029, 102], [101, 2054, 2003, 1996, 2561, 2193, 1997, 3152, 2007, 1996, 2653, 1997, 13873, 3205, 1029, 102], [101, 2054, 2001, 1996, 2193, 1997, 2111, 7052, 1996, 23790, 2015, 3290, 5443, 1012, 26843, 5956, 2208, 1029, 102], [101, 2054, 2051, 2558, 2018, 2053, 3797, 10460, 1029, 102], [101, 2043, 2001, 2010, 2034, 3083, 2173, 2501, 1029, 102], [101, 2515, 6986, 2030, 2198, 2031, 1996, 3284, 2561, 1029, 102], [101, 2054, 2003, 1996, 4117, 3556, 1997, 2095, 2203, 10385, 2077, 2268, 1029, 102], [101, 2129, 2116, 2062, 3719, 2020, 18480, 1999, 2697, 21899, 2084, 1999, 13374, 1029, 102], [101, 2054, 2001, 1996, 2561, 2193, 1997, 2685, 3195, 2011, 1996, 10401, 1999, 1996, 2197, 1017, 2399, 4117, 1012, 102], [101, 2040, 2234, 3202, 2044, 6417, 13809, 1999, 1996, 2679, 1029, 102], [101, 2054, 1005, 1055, 1996, 2561, 2193, 1997, 7519, 2008, 4158, 1999, 2255, 1029, 102], [101, 2054, 2003, 1996, 2561, 2193, 1997, 15315, 13390, 3765, 2853, 1999, 1996, 2095, 2384, 1029, 102], [101, 2054, 2001, 1996, 2193, 1997, 2335, 2180, 2006, 5568, 1029, 102], [101, 2040, 2180, 1996, 2087, 2751, 6665, 1029, 102], [101, 2561, 5222, 2011, 6995, 8195, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'labels': [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['test'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Lookup\", \"Aggregation\"]\n",
    "Num_labels = len(labels)\n",
    "id2label = {id:label for id, label in enumerate(labels)}\n",
    "label2id = {label:id for id, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Lookup', 1: 'Aggregation'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lookup': 0, 'Aggregation': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred = (predictions, labels)\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 다중분류\n",
    "    # 특정 i라벨의 확률 = 특정 i 라벨의 승산/모든 라벨의 승산 \n",
    "    # predictions = [batch_size, num_labels]\n",
    "    #probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "\n",
    "    #positive_class_probs = probabilities[:, 1] # 클래스 1일 확률 \n",
    "\n",
    "    # compute auc\n",
    "    #auc = np.round(auc_score.compute(prediction_scores=positive_class_probs,\n",
    "    #                reference=labels)['roc_auc'],3)\n",
    "\n",
    "    # 가장 로짓이 큰 라벨 추출\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # compute accuracy\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, \n",
    "                                     references=labels)['accuracy'],3)\n",
    "    \n",
    "    f1 = np.round(f1_score.compute(predictions=predicted_classes, references=labels, average='macro')['f1'], 3) #  라벨별 f1-score를 산술평균한 것 : 현재 라벨의 갯수가 같아서 이렇게 써도 된다고 판단\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 3e-5\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/B6313/tableqa_wiki/bert-agg',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    # Number of steps used for a linear warmup\n",
    "    warmup_steps=2800,                \n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    fp16=True,\n",
    "    #metric_for_best_model=\"f1\",\n",
    "    dataloader_num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B6313\\AppData\\Local\\Temp\\ipykernel_16584\\4180147362.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21849' max='55790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21849/55790 16:44 < 26:00, 21.75 it/s, Epoch 3.92/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.294360</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.299040</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.681470</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\transformers\\trainer.py:2593\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2588\u001b[39m     _grad_norm = nn.utils.clip_grad_norm_(\n\u001b[32m   2589\u001b[39m         amp.master_params(\u001b[38;5;28mself\u001b[39m.optimizer),\n\u001b[32m   2590\u001b[39m         args.max_grad_norm,\n\u001b[32m   2591\u001b[39m     )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     _grad_norm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2599\u001b[39m     is_accelerate_available()\n\u001b[32m   2600\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED\n\u001b[32m   2601\u001b[39m ):\n\u001b[32m   2602\u001b[39m     grad_norm = model.get_global_grad_norm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\accelerate\\accelerator.py:2610\u001b[39m, in \u001b[36mAccelerator.clip_grad_norm_\u001b[39m\u001b[34m(self, parameters, max_norm, norm_type)\u001b[39m\n\u001b[32m   2608\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m model.clip_grad_norm_(max_norm, norm_type)\n\u001b[32m   2609\u001b[39m \u001b[38;5;28mself\u001b[39m.unscale_gradients()\n\u001b[32m-> \u001b[39m\u001b[32m2610\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:215\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    213\u001b[39m     parameters = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m    214\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\B6313\\miniconda3\\envs\\wiki_cls\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:98\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m         norms.extend(\n\u001b[32m     94\u001b[39m             [torch.linalg.vector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     97\u001b[39m total_norm = torch.linalg.vector_norm(\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, norm_type\n\u001b[32m     99\u001b[39m )\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    103\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for gradients from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mset `error_if_nonfinite=False`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "logits = preds.predictions\n",
    "labels = pred.label_ids\n",
    "\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
